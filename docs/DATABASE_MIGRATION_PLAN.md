# ğŸ“Š Aktualizacja struktury bazy danych GPW Scraper v2.0

## ğŸ”„ ZMIANY W STOSUNKU DO STAREJ WERSJI

### âœ… NOWE TABELE:

1. **`scheduled_jobs`** - Konfiguracje zadaÅ„ cron
   - ZastÄ™puje pliki JSON w `/configs/`
   - Przechowuje wszystkie parametry harmonogramu
   - Åšledzi statystyki wykonaÅ„ (`last_run`, `next_run`, `run_count`)
   - Wsparcie dla JSON: `report_types`, `report_categories`, `tags`

2. **`summary_reports`** - Zbiorcze raporty MD/PDF
   - ÅšcieÅ¼ki do plikÃ³w `.md` / `.pdf`
   - Metadata: liczba raportÃ³w, dokumentÃ³w, rozmiar
   - PodglÄ…d (pierwsze 500 znakÃ³w)
   - System tagÃ³w i archiwizacji

3. **`job_execution_log`** - Historia wykonaÅ„ zadaÅ„
   - Status: success, failed, running, cancelled
   - Czas wykonania, liczba przetworzonych dokumentÃ³w
   - PowiÄ…zanie z wygenerowanym raportem zbiorczym
   - ÅšcieÅ¼ka do pliku logu

4. **`downloaded_files`** - Rejestr pobranych plikÃ³w
   - Åšledzenie wszystkich PDF/HTML/CSV
   - Hash MD5 do wykrywania duplikatÃ³w
   - Przechowywanie podsumowaÅ„ AI dla pojedynczych plikÃ³w
   - PowiÄ…zanie z raportami GPW

### ğŸ”§ ULEPSZONE TABELE:

1. **`firma`**
   - â• `pelna_nazwa` - peÅ‚na nazwa firmy
   - â• `sektor` - sektor gospodarki (Technologia, BankowoÅ›Ä‡, itd.)
   - â• `data_dodania` - timestamp dodania

2. **`dane`**
   - â• `id_raportu` - PRIMARY KEY auto-increment
   - â• `data_pobrania` - kiedy raport zostaÅ‚ pobrany
   - âœ… Indeksy na: `data`, `typ_raportu`, `kategoria_raportu`
   - âœ… CASCADE delete przy usuniÄ™ciu firmy

3. **`historia`**
   - â• `model_used` - jaki model AI byÅ‚ uÅ¼yty
   - â• `execution_time` - czas wykonania scrapingu
   - â• `data_wyszukiwania` - timestamp
   - âœ… Indeksy na: `company_name`, `data_wyszukiwania`

### ğŸ“Š NOWE WIDOKI:

1. **`v_active_jobs`** - Aktywne zadania z ostatnim statusem
   ```sql
   SELECT * FROM v_active_jobs WHERE enabled = TRUE;
   ```

2. **`v_company_stats`** - Statystyki per firma
   ```sql
   SELECT * FROM v_company_stats ORDER BY total_reports DESC;
   ```

### âš™ï¸ PROCEDURY SKÅADOWANE:

1. **`update_job_stats()`** - Aktualizacja statystyk po wykonaniu zadania
   ```sql
   CALL update_job_stats('weekly_asseco', 'success', 15, 8, 120, 42);
   ```

---

## ğŸ¯ KORZYÅšCI Z NOWEJ STRUKTURY:

### 1. **PeÅ‚na integracja z systemem harmonogramÃ³w**
   - âŒ Koniec z plikami JSON w `/configs/`
   - âœ… Wszystko w bazie danych
   - âœ… Åatwe zarzÄ…dzanie przez UI

### 2. **Åšledzenie historii wykonaÅ„**
   - Kiedy, ile razy, jaki status
   - PowiÄ…zanie logu â†’ raport zbiorczy
   - Wykrywanie bÅ‚Ä™dÃ³w

### 3. **Deduplikacja plikÃ³w**
   - Hash MD5 kaÅ¼dego pobranego pliku
   - OszczÄ™dnoÅ›Ä‡ miejsca
   - Unikanie wielokrotnego przetwarzania

### 4. **Lepsza wydajnoÅ›Ä‡ zapytaÅ„**
   - Indeksy na wszystkich czÄ™sto uÅ¼ywanych kolumnach
   - Widoki dla skomplikowanych zapytaÅ„
   - Foreign keys z CASCADE

### 5. **RozszerzalnoÅ›Ä‡**
   - JSON dla elastycznych danych (tagi, config)
   - LONGTEXT dla podsumowaÅ„ AI
   - ENUM dla typÃ³w (Å‚atwo rozszerzyÄ‡)

---

## ğŸ“ MIGRACJA - CO TRZEBA ZAKTUALIZOWAÄ† W KODZIE:

### 1. **`database_connection.py`**
```python
# NOWE funkcje do dodania:

def wstaw_scheduled_job(job_name, company, date_from, date_to, model, cron_schedule, enabled, report_types, report_categories)
def aktualizuj_scheduled_job(job_id, **kwargs)
def pobierz_scheduled_jobs(enabled_only=False)
def usun_scheduled_job(job_name)

def wstaw_job_execution_log(job_name, status, started_at, finished_at, duration, reports_found, docs_processed, summary_report_id, error_msg, log_path)
def pobierz_ostatnie_wykonanie(job_name)

def wstaw_downloaded_file(company, report_id, file_name, file_path, file_type, file_size, md5_hash, summary_text)
def sprawdz_czy_plik_istnieje(md5_hash)
def aktualizuj_podsumowanie_pliku(file_id, summary_text)

# ZAKTUALIZOWANE:
def wstaw_historie(..., model_used, execution_time)  # + 2 parametry
```

### 2. **`config_manager.py`**
```python
# MIGRACJA z JSON â†’ baza danych
# Opcja 1: ZachowaÄ‡ JSON jako backup
# Opcja 2: CaÅ‚kowicie przenieÅ›Ä‡ do bazy

class ConfigManager:
    def __init__(self, use_database=True):  # Nowy parametr
        if use_database:
            # UÅ¼yj database_connection
        else:
            # Stary system JSON
```

### 3. **`cron_manager.py`**
```python
# Po instalacji do crontab â†’ aktualizuj next_run w bazie
def install_jobs(self):
    # ... istniejÄ…cy kod ...
    # DODAJ:
    db.aktualizuj_next_run(job_name, next_run_timestamp)
```

### 4. **`run_scheduled.py`**
```python
# Na poczÄ…tku zadania:
db.wstaw_job_execution_log(job_name, 'running', datetime.now(), ...)

# Na koÅ„cu:
db.aktualizuj_job_execution_log(log_id, status='success', finished_at=..., summary_report_id=...)

# Przy kaÅ¼dym pobranym pliku:
md5 = calculate_md5(file_path)
if not db.sprawdz_czy_plik_istnieje(md5):
    db.wstaw_downloaded_file(...)
```

### 5. **`scrape_script.py`**
```python
# Po pobraniu pliku:
md5_hash = hashlib.md5(open(file_path, 'rb').read()).hexdigest()
wstaw_downloaded_file(..., md5_hash=md5_hash)

# Przy zapisie zbiorczego raportu:
wstaw_zbiorczy_raport(..., document_count=len(files), summary_preview=summary[:500])
```

---

## ğŸš€ INSTALACJA NOWEJ BAZY:

### Krok 1: Backup starej bazy
```bash
mysqldump -u user -pqwerty123 "gpw data" > gpw_data_backup_$(date +%Y%m%d).sql
```

### Krok 2: UsuÅ„ i stwÃ³rz nowÄ…
```bash
mysql -u user -pqwerty123 < gpw_data_v2.sql
```

### Krok 3: Migruj dane (opcjonalnie)
```bash
# JeÅ›li chcesz zachowaÄ‡ stare dane:
mysql -u user -pqwerty123 "gpw data" -e "
INSERT INTO firma (nazwa) SELECT DISTINCT company_name FROM historia_old;
"
```

---

## â“ PYTANIA DO ROZWAÅ»ENIA:

1. **Czy zachowaÄ‡ pliki JSON w `/configs/` jako backup?**
   - âœ… Zaleta: BezpieczeÅ„stwo, Å‚atwy rollback
   - âŒ Wada: Duplikacja danych

2. **Czy przechowywaÄ‡ peÅ‚ne podsumowania w `downloaded_files.summary_text`?**
   - âœ… Zaleta: Szybki dostÄ™p, moÅ¼liwoÅ›Ä‡ re-generacji zbiorczego raportu
   - âŒ Wada: DuÅ¼a baza (LONGTEXT)
   - ğŸ’¡ Propozycja: Tak, ale z moÅ¼liwoÅ›ciÄ… archiwizacji starych

3. **Czy automatycznie przenosiÄ‡ stare raporty do archiwum?**
   - ğŸ’¡ Propozycja: Po 90 dniach `is_archived = TRUE`
   - Dedykowana funkcja w UI: "PokaÅ¼ archiwum"

4. **Czy synchronizowaÄ‡ `scheduled_jobs` z crontab?**
   - âœ… Zaleta: Jednoznaczne ÅºrÃ³dÅ‚o prawdy
   - ğŸ’¡ Propozycja: Przy kaÅ¼dym `install_jobs()` â†’ INSERT/UPDATE w bazie

---

## âœ… TODO PRZED MIGRACJÄ„:

- [ ] PrzejrzeÄ‡ i zatwierdziÄ‡ strukturÄ™ `gpw_data_v2.sql`
- [ ] ZdecydowaÄ‡ o strategii migracji danych
- [ ] ZaktualizowaÄ‡ wszystkie funkcje w `database_connection.py`
- [ ] ZaktualizowaÄ‡ `config_manager.py` (baza vs JSON)
- [ ] DodaÄ‡ obsÅ‚ugÄ™ logÃ³w w `run_scheduled.py`
- [ ] DodaÄ‡ hash MD5 w `scrape_script.py`
- [ ] PrzetestowaÄ‡ nowÄ… bazÄ™
- [ ] UtworzyÄ‡ backup starej bazy
- [ ] WykonaÄ‡ migracjÄ™
