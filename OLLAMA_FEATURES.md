# Nowe funkcje - ZarzÄ…dzanie modelami Ollama

## Co zostaÅ‚o dodane?

### 1. ModuÅ‚ `ollama_manager.py`
Nowy moduÅ‚ zarzÄ…dzania modelami Ollama z nastÄ™pujÄ…cymi funkcjami:

- `is_ollama_available()` - sprawdza, czy Ollama jest zainstalowane
- `get_installed_models()` - zwraca listÄ™ zainstalowanych modeli
- `is_model_installed(model_name)` - sprawdza, czy konkretny model jest zainstalowany
- `pull_model(model_name, progress_callback)` - pobiera model z obsÅ‚ugÄ… callbacku postÄ™pu
- `get_available_models()` - lista rekomendowanych modeli do streszczania (< 10GB)
- `get_model_display_name(model_name, is_installed)` - formatuje nazwÄ™ dla UI

### 2. Rozszerzone UI (`user_interface.py`)
Do interfejsu Gradio dodano:

- **Dropdown wyboru modelu** z oznaczeniami:
  - âœ“ = model zainstalowany
  - â—‹ = model do pobrania
- **Przycisk odÅ›wieÅ¼ania** (ğŸ”„) - aktualizuje listÄ™ modeli po rÄ™cznym zainstalowaniu
- **Automatyczne pobieranie** - jeÅ›li wybrany model nie jest zainstalowany, UI automatycznie go pobierze przed rozpoczÄ™ciem scrapingu
- **PostÄ™p pobierania** - wyÅ›wietlanie informacji o postÄ™pie pobierania modelu

### 3. Zmodyfikowane moduÅ‚y
- `summary.py` - funkcja `summarize_document_with_kmeans_clustering()` przyjmuje teraz parametr `model_name`
- `scrape_script.py` - funkcje `get_summaries()` i `scrape()` przyjmujÄ… parametr `model_name`

## Rekomendowane modele (< 10GB)

Skonfigurowane w `ollama_manager.py`:

1. **llama3.2:latest** (~6GB) - doskonaÅ‚y do streszczania, dobry balans wydajnoÅ›Ä‡/rozmiar
2. **llama3.2:3b** (~2GB) - mniejszy wariant
3. **mistral:latest** (~4GB) - dobry kompromis
4. **phi3:latest** (~2.3GB) - efektywny model Microsoft
5. **gemma:7b** (~5GB) - model Google
6. **qwen2:7b** (~4.4GB) - model Alibaba

## Jak uÅ¼ywaÄ‡?

### W interfejsie graficznym:
1. Uruchom aplikacjÄ™: `python user_interface.py`
2. W sekcji z parametrami znajdziesz dropdown "Ollama Model"
3. Wybierz model z listy:
   - JeÅ›li ma âœ“ - jest gotowy do uÅ¼ycia
   - JeÅ›li ma â—‹ - zostanie automatycznie pobrany przy klikniÄ™ciu "Run"
4. Kliknij przycisk ğŸ”„ aby odÅ›wieÅ¼yÄ‡ listÄ™ modeli (np. po rÄ™cznej instalacji)

### Automatyczne pobieranie:
Gdy wybierzesz model niepobrany i klikniesz "Run":
1. UI wyÅ›wietli komunikat o rozpoczÄ™ciu pobierania
2. Pobieranie postÄ™pu bÄ™dzie widoczne w sekcji "Output"
3. Po zakoÅ„czeniu pobierania automatycznie rozpocznie siÄ™ scraping

### Programowo (Python):
```python
from ollama_manager import pull_model, is_model_installed

# SprawdÅº, czy model jest zainstalowany
if not is_model_installed("llama3.2:latest"):
    success, message = pull_model("llama3.2:latest")
    print(message)

# UÅ¼yj modelu w streszczaniu
from summary import summarize_document_with_kmeans_clustering
summary = summarize_document_with_kmeans_clustering("path/to/file.pdf", "llama3.2:latest")
```

## Uwagi techniczne

- Pobieranie modelu moÅ¼e zajÄ…Ä‡ kilka minut w zaleÅ¼noÅ›ci od rozmiaru i prÄ™dkoÅ›ci internetu
- UI uÅ¼ywa generatora (yield) do pokazywania postÄ™pu pobierania bez blokowania interfejsu
- Funkcja `run_scrape_ui()` najpierw sprawdza dostÄ™pnoÅ›Ä‡ modelu, pobiera go jeÅ›li trzeba, a potem uruchamia scraping
- Parametr `num_predict` zastÄ…piÅ‚ `max_tokens` dla zgodnoÅ›ci z najnowszym API ChatOllama

## Testowanie

SprawdÅº dostÄ™pnoÅ›Ä‡ Ollama:
```bash
ollama --version
```

Lista zainstalowanych modeli:
```bash
ollama list
```

RÄ™czne pobieranie modelu:
```bash
ollama pull llama3.2:latest
```
